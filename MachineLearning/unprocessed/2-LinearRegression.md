---
layout: notes
title: Linear Regression
class: Intro to Machine Learning
---

# Linear Regression
---

Linear Regression is one of the most common applications of machine learning
* Given a data set, coming up with a function that best maps _x_ to __y__


### Terminology
_m_: The number of training examples
_x_: The features (or the number of features)
__y__: The target variable (aka the output)
_Hypothesis_: A function that describes the data

### Hypothesis
A hypothesis is a function that seeks to describe a set of data
* It is the function that generates a best fit line
* Equation
$$
h_\theta(x)= \theta_0 + θ_1x_1+ θ_2x_2 + θ_3x_3 + ... + θ_nx_n
$$

This can also be written as:

$$
\begin{bmatrix}
\theta_0 & \theta_1 & \theta_2 & ... & \theta_n
\end{bmatrix}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
... \\
x_n \\
\end{bmatrix}
= \theta^T x
$$

### Cost Function
Let us define a cost function to be the difference between the expected value, and the value generated by the hypothesis
* Thus, the best fit line is the line with the least cost function, i.e. J(x) is closest to 0

Usually, we use the cost function:
$$
J(\theta_1, \theta_2) = \frac{1}{2m} \sum_{i = 1}^{m} (h_\theta(x_i) - y_i)^2
$$

### Gradient Descent
To algorithmically generate a better hypothesis (this is machine _learning_ after all), we use gradient descent
* __Definition__: Following the negative of the gradient (derivative at a point) to get to the local minima
* Algorithm:
$$
\theta_j := \theta_j − \alpha \frac{1}{m} \sum_{i = 1}^{m} (h_\theta(x^{i}) - y^i) * x_j^i
$$

where α is the learning rate

### Learning Rate
As mentioned in the previous section, α is the learning rate, i.e. the amount that the function "moves" after each iteration of gradient descent. Picking a correct value for α is very important
* If α is too small, it will take too long for J(θ) to converge
* If α is too large, J(θ) might not decrease on every iteration, and thus diverge from the extrema

From these two observations, we can determine two rules to follow to choose a good learning rate
1. If J(θ) ever increases between any two iterations, decrease α
2. If J(θ) decreases by less than some value E between two iterations (where E is some small number), declare convergence or increase α

### Feature Scaling
Since we are doing this iteratively, there might be no limit on how long it takes for gradient descent to work. To make it go faster, it is best to _normalize_ the ranges of the features, i.e.

$$
-1 \leq x_i \leq 1
$$

or

$$
-0.5 \leq x_i \leq 0.5
$$

There are two main methods for doing so:
1. __Feature Scaling__: Dividing the feature by the range (normalizes the features to a range of 1)
2. __Mean Normalization__: Subtracting the mean of the feature set from the feature (normalizes the features to a mean of 0)

Thus, after feature scaling, the input set would have values generated with the following function

$$
x_i := \frac{x_i - \mu_i}{s_i}
$$

where µ is the mean, and _s_ is the range

### Polynomial Regression
If a line does not fit the data set well, we can use other shapes as well, making features quadratic, squares, cubics, or any form we choose.

In this case, it is important to do feature scaling, as if x_1 has range 1 - 1000, x_1^2 has range 1 - 1000000, and so on